{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label_encoder_age_Salary :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER TECHNIQUE -\n",
    "* Any data which is in categorycal data we can convert into the numeric with the help of encoding technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encoding has two categorey :-\n",
    "    * 1 > One hot encoder -It is work for the only two categorey of data (yes or now or M or F etc it will convert m=0 and f=1 automatically) one hot encoder work only for the binery data only means two categorical data should be present the perticular columns.\n",
    "    * 2 > Label encoder -Inside the Label encoder we can convert any number of category(country names etc it will coverted into numerical data) and label encoder work for more than two categorical data.\n",
    "    * whatever categorical data is present it will just give the number to it  in case of label encoder. it is going to do labeling the categorical data.\n",
    "    * whenever we have dataset with categorical data set instead of droping that columns just change it into numierical with the help of label encoder we can take it.\n",
    "    * label encoder is the best one because as many as categorey are present it will start giving number to those one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling label encoder and OneHotEncoder\n",
    "# we can apply any of them OnehotEncoder and label encoder from the preprocessing here.\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset \n",
    "\n",
    "ds=pd.read_csv(\"age_salary.csv\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see above dataset we can apply onehotencoder for the purchased_item and labelencoder for Nation columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see information of entire dataset\n",
    "\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to know null value are present or no in dataset\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum of null value showing of each colums of our dataset\n",
    "ds.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Note:-__\n",
    "\n",
    "* __Technique to drop or fill null value:-__\n",
    "\n",
    "    * ds.dropna this will drop the all null value.\n",
    "    * Instead of drop null value or lossing information we have to find mean,median, mode of that perticular column and fill that value at place of null value.\n",
    "    * wherever the numerical data is present we can apply the mean and the median to fill null value  instead of droping those.\n",
    "    * and whereever the charecter data is present or object data is present so we can apply the mode value to fill the na/null value instead of droping those.\n",
    "    * Inside the pandas all the method are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['age']=ds['age'].fillna((ds['age'].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are replace method to here we find mean of the salary column and we can replace the nan value into mean value.\n",
    "\n",
    "import numpy as np\n",
    "ds=ds.replace(np.NaN,ds['salary'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Imputer Function/Simple Imputer :-__\n",
    "\n",
    "* Imputer web link - https://scikit-learn.org/0.16/modules/generated/sklearn.preprocessing.Imputer.html\n",
    "\n",
    "\n",
    "* from sklearn.preprocessing import Imputer \n",
    "* imp=Imputer (missing_value=np.NaN,strategy=\"mean\") #strategy = mean means kindly handle handle the null value by finding out the mean of that columns.\n",
    "* ds['age']=imp.fit_transform(ds['age']) #here we provide the column names,here we are making fit_transform method which will  allow me to find out the mean and it will fill mean value of the age into the nan value.\n",
    "* ds\n",
    "\n",
    "\n",
    "* above given simlar way we do for for median value also.\n",
    "\n",
    "\n",
    "* from sklearn.preprocessing import Imputer \n",
    "* imp=Imputer (missing_values=np.NaN,strategy=\"median\")\n",
    "* ds['salary']=imp.fit_transform(ds['salary'])\n",
    "* ds\n",
    "\n",
    "\n",
    "* from sklearn.preprocessing import Imputer \n",
    "* imp=Imputer (missing_value=np.NaN,strategy=\"most_frequent\")   #most_freqquent using for object/string data\n",
    "* ds['Nation']=imp.fit_transform(ds['Nation'])\n",
    "* ds \n",
    "\n",
    "* Note - we can know Mean, Median, Mode  and Most_frequent use it from simple imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling Label encoder from preprocessing \n",
    "le=LabelEncoder()\n",
    "ds['Nation']=le.fit_transform(ds['Nation']) #le.fit_transform method wich will allow character data(nation) into the numerical format.\n",
    "ds['Nation']\n",
    "\n",
    "# In output 0,1,2 it means three nation is there.if four it will like 0,1,2,3,4...and so on.(see right column not index wala ).\n",
    "# And every number represent country name is here that will repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds\n",
    "\n",
    "# Now we can see nation is into the numerical so we have not drop the nation.\n",
    "# For purchase item we can use oneahotencoder or Labelencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=ds.iloc[:,-1].values\n",
    "# here we are taking all the row and 0,1,3,4(index,nation,salary, age) columns to the x by iloc function so i gettion all the numerical data.\n",
    "#but 2th (purchase_item) column having the data, means yes or no\n",
    "\n",
    "x=ds.iloc[:,[0,1,3,4]]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are extracting the 2th column with this method... \n",
    "y=ds.iloc[:,2].values\n",
    "y\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping \n",
    "y=y.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appling labelencoder for y\n",
    "\n",
    "one=LabelEncoder()\n",
    "y=one.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__we can do machine learning processing from here for this data set....__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Dataset, label_encoder_drop :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_ csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=pd.read_csv('empl.csv')\n",
    "ds\n",
    "\n",
    "#in this dataset name not important we can drop this column and generally name is not important.\n",
    "# but here in the city and country we can apply labelencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking data into dataframe :-\n",
    "df=pd.DataFrame(data=ds)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical summery of all numrical data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to know data type\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droping name columns\n",
    "\n",
    "df1=df.drop(['Name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "# orignal one is still having name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are deleting orignal Names column and axis =1 means drop the columns\n",
    "df.drop(['Name'],axis=1,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null value \n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of null value\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dropna(axis=0,iplace=True)\n",
    "#By this method we can drop entire row with null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note-\n",
    "* class sklearn.impute.SimpleImputer(*, missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
    "* Imputation transformer for completing missing values.\n",
    "\n",
    "* Read more in the User Guide.\n",
    "\n",
    "* *New in version 0.20: SimpleImputer replaces the previous sklearn.preprocessing.Imputer estimator which is now removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn-pandas\n",
    "\n",
    "#sklearn-pandas has categoricalImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can handle null value by using simple imputer method insted of droping null value\n",
    "\n",
    "# sklearn.impute simpleImputer class works for imputing null values in object or categorical data or object type\n",
    "\n",
    "# calling simpleimputer method\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(strategy =\"most_frequent\") #most frequent use for charater datatypes\n",
    "\n",
    "df['City']=imp.fit_transform(df['City'].values.reshape(-1,1)) #value.reshape here means give the data into the format.\n",
    "\n",
    "df\n",
    "\n",
    "# we can see below into the nan value hongkong is appearing more means nan value fill with hongkong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Age']=df['Age'].values.reshape(-1,1)\n",
    "#df['Age'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are replace method to here we find mean of the salary column and we can replace the nan value into mean value.\n",
    "\n",
    "import numpy as np\n",
    "df=df.replace(np.NaN,df['Salary'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.replace(np.NaN,df['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputer works for numerical data such as Age and salary\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp =SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df['Age']=imp.fit_transform(df['Age'].values.reshape(-1,1))\n",
    "df['Salary']=imp.fit_transform(df['Salary'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can use LableEncoder for  multiple columns by making list  \n",
    "\n",
    "le=LabelEncoder()\n",
    "# df['City']=le.fit_transform(df['city'])\n",
    "\n",
    "list1=['City','Country'] #making list\n",
    "for val in list1:            #and rotating into the for loop one by one country,city  list will come and then it will transform by given below line code\n",
    "    df[val]=le.fit_transform(df[val].astype(str))  #astype(str) means certain time before throwing any error it will forcefully  put all the data as a string transform the data into the labelencoder(category into numerical) and then it will be transform.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we can see 4, 11 and 13 rows are filled with 9 . (Null is replace by 9)\n",
    "\n",
    "Replace 9 which NaN and Replace it with most frequent value."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy\n",
    "\n",
    "df['City'].replace(9,numpy.NaN,inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#['mean','median','most_frequent']\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp=Imputer(missing_value='NaN',strategy='most_frequent')\n",
    "df['City']=imp.fit_transform(df['City'].values.reshape(-1,1))\n",
    "\n",
    "print(df)\n",
    "\n",
    "Method :-\n",
    "\n",
    "#for column:-\n",
    "# whenever i want to replace the NaN value of a perticular column every time we have to dataframe(df) into the column name(any).\n",
    "# .replace method replace all the nan value to zero or any provided value  for  any perticular column.\n",
    "# we have mask option in padas to handle -1 value will study later.\n",
    "\n",
    "df['column']=df['column'].replace(np.nan,0)\n",
    "solution\n",
    "\n",
    "df['age']=df['age'].replace(np.Nan,df['age'].mean())\n",
    "\n",
    "# for whole dataframe \n",
    "# \n",
    "df=df.replace.(np.nan,0)\n",
    "\n",
    "df=df.replace(-1,df.mean())\n",
    "\n",
    "# inplace\n",
    "#instead of nan certain time having -1 value (very rare cases).\n",
    "#so we replace -1 value into np.nan.\n",
    "# or we can findout the mean value to just replace the -1 value.\n",
    "df.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "\n",
    "df.replace(-1,np.Nan,inplace=True)\n",
    "df.replace(-1,df.mean(),inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#filling strings : when string columns have missing values and NaN values.\n",
    "\n",
    "df.['string colum name'].fillna()df['string column name'].mode().value[0],inplace=True #filling mode voulue(for str data type) to the nan value.\n",
    "\n",
    "#filling numeric columns: when the numeric columns have missing values and NaN values.\n",
    "\n",
    "df['column name'].fillna(df['numeric column name'].mean(,inplace = True)) #filling mean value(for numeric data type) to the nan value.\n",
    "\n",
    "#filling Nan With zero\n",
    "\n",
    "df['column name'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__from here we do machine learning process....__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SimpleImputer\n",
    "# for cols in ['City'],['Country']: #here both are quantitative features.\n",
    "#     df[cols] = impute.fit_transform(xx[[cols]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.iloc[:,0:4]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df.iloc[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx,testx,trainy,testy=train_test_split(x,y,test_size=.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* During the learning/training the score is ok but at the time of testing it is showing the ans somewhere else it means it showing the answer apart from the target.\n",
    "* when the model is starting the biasing internally into the regression so it start giving y ans own it own.it start giving the daviated answer on to one position only, answer are away from the actual answer but all the answer are into the same position.So this happend into the biasness into the model.\n",
    "* Certain time model is start puting the variances also. with the high bias and high variance it starting giving the answer away from the the actual answer and all the answer are variances as well high variences in answers. means dispertion of the answer are here.\n",
    "* Certain time the model start working , Into the low bias and if the model is working the high variance it means training time it will working ok but when we giving newer data to testing it will start giving the answer neearby to the actual target near by the actual target but it will giving  varience in answers.\n",
    "* Low Bias and Low Variace- If any model less bias and less variance it starti hiting the right answers and all are into the right predicted answers  or giving answers at the target position.\n",
    "* Low Bias and High Variance- whenever model start behaving suh as low bias and high variance it start giving the answer the  nearby the target but answer is away from the actual positing of answere dispertion of answer.\n",
    "* High Bias low Variance- Whenever the model start behaving such as high bias and low variance such as away  answer is predicted at same position from the actual position of answer.\n",
    "* High bias and High variance- whenever the model is start behaving such as  high bias and high varience such as awey answer is here from actual position of answer.\n",
    "\n",
    "* So we call above all these situation as the unlderfitting and the overfiting.\n",
    "* Underfitting - In underfit high bias is there here model will bypass everything  and it start making its own bias and it was start making own assumption for the answer.This is the case of high bias.this means model is not learning all the data.\n",
    " \n",
    "* Overfitting - In overfitting the model is start learning all the data during the training period  and when we give the new testing data again its answer are to munch deviated we call it the stage of overfittng.\n",
    "\n",
    "* Certain time underfitting and overfitting happen happened means that is not good for model.\n",
    "\n",
    "* underfitting and overfitting happen also with Regression and Classification as well so underfitting it randomally making any classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Underfitting and Overfitting in Machine Learning link -__ https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/\n",
    "\n",
    "* __Underfitting:-__\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data. (It’s just like trying to fit undersized pants!) Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. It usually happens when we have less data to build an accurate model and also when we try to build a linear model with a non-linear data. In such cases the rules of the machine learning model are too easy and flexible to be applied on such minimal data and therefore the model will probably make a lot of wrong predictions. Underfitting can be avoided by using more data and also reducing the features by feature selection.\n",
    "\n",
    "* __Overfitting:-__\n",
    "A statistical model is said to be overfitted, when we train it with a lot of data (just like fitting ourselves in oversized pants!). When a model gets trained with so much of data, it starts learning from the noise and inaccurate data entries in our data set. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  __There are techniques to  control the overfitting and underfitting.__\n",
    "* There is One technique that one is the regularization.\n",
    "* Regularization is the method Which convince you to handle the underfitting and the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization :-\n",
    "* There are algorithms L1(Lasso) and L2(Ridge Regression).\n",
    "* Lasso(L1) and Redge Regression are those Regression which will control the Underfitting and overfitting into the data.\n",
    "\n",
    "\n",
    "* __Lasso Regression-__ Inside the lasso alpha parameter(alpha=lprovide value) which will control the cofficient value given to the Y. here cofficient means ome of the column working negatively and some of the column woking positively.certain time column cofficient working is high positve and high negative which is influencing the prediction and target variable.so Lasso will do  the alpha parameter will control this cofficient value. so it is try to minimize the diferece between the cofficient value so that it will start working the right answer.\n",
    "* Certain time lasso reduce/omit the cofficient to the zereo during the traning period because it think at this column is not very use full for the target variable. so certain time omit or make the columns cofficient zero also it means this  column will not work for the prediction of the y variable.So this is control by the alpha parameter during the alpha parameter we provide small value to it such like(alpha= 0.00001,0.001,0.001,0.01,0.1,0.10 here 0.1 and 0.10 is the highest value) then checking the r2 score how our score improving by changing the alpha value.\n",
    "\n",
    "* Here we are changing alpha hyperparameter tuning like 0.00001,0.001,0.001,0.01,0.1 so\n",
    "\n",
    "* Basically Lasso is omit the value or omit the that perticular column's cofficient.\n",
    "\n",
    "* __Ridge Regression-__ Ridge regression will minimize the difference between the Two Columns's cofficient.\n",
    "\n",
    "* __Elasticness regression:-__ Inside the elasticness regression abobe both of the things are available that is combition of both lasso and ridge. This model will decide which column should be zero which cofficient should be zero or which cofficient should be reduce come less defference.\n",
    "\n",
    "* All this thigs come under the linear Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BostonLinearRegression_Lasso_Ridge_regressor :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston=load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=boston.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=33,random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(boston.feature_names,lm.coef_)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos=pd.DataFrame(boston.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos.columns=boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bos['B'],y)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bos['RM'],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bos['CRIM'],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(list(zip(bos.column,lm.coef_)),columns=['feature','estimated coeffiient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the value \n",
    "\n",
    "pred=lm.predict(x_test)\n",
    "print(\"Predicted Result price:\",pred)\n",
    "print(\"actual price\",y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('error')\n",
    "print(mean_squared_error(pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the house price by providing the values\n",
    "td=np.array([0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.0900,1.0,296.0,15.3,396.90,4098])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td=td.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.predict(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lmscore=cross_val_score(lm,x,y,cv=8)\n",
    "lmscore\n",
    "print(lmscore.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization :: L1 and L2 Regularization\n",
    "L1---------->Lasso Regression\n",
    "\n",
    "L2---------->Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling lasso and ridge regression\n",
    "from sklearn.linear_model import Lasso,Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will reduce the cofficient o zero (those feature are not information)\n",
    "#alpha values  could be ----->0.0001,0.001,0.01,0.1,10--------->higher value reduce all coefficients towards 0 and impact out out \n",
    "#Default Value of alpha is 1.0\n",
    "#alpha=.01\n",
    "ls=Lasso(alpha=0.0001) #changing of the alpha value should be very small\n",
    "# ls= Lasso(alpha=1.0) # Default\n",
    "ls.fit(x_train,y_train)#making the model fit c\n",
    "ls.score(x_train,y_train)# checking out the score how model id working well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.coef_ #checking out the cifficient value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(boston.feature_names,ls.coef_) # ploting cofficient \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to minimize the coefficient varience\n",
    "\n",
    "rd=Ridge(alpha=0.0001)\n",
    "#rd=Ridge()\n",
    "rd.fit(x_train,y_train)\n",
    "rd.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(boston.feature_names,rd.coef_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet is a combination of both Lasso and Ridge read below documentation\n",
    "\n",
    "from sklearn.linear_model import ElasticNet #calling elasticnet\n",
    "enr=ElasticNet(alpha=0.0001)\n",
    "#enr=ElasticNet()\n",
    "enr.fit(x_train,y_train)\n",
    "enrpred=enr.predict(x_test)\n",
    "print(enr.score(x_train,y_train))\n",
    "enr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know support vector machine (SVM).SVR ,KNN,Decission tree is for the regression purpose also.\n",
    "#  read below documentation link :-\n",
    "# SVR-------->https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "# Linear Models---->https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "#  KNR------------->https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "\n",
    " # Basic method -\n",
    "    # in this method  calling SVR and we are making saperately kernel value and testing by putting  different kernel values  (Linear,poly,rbf) one by one saperetely see below.\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr=SVR(kernel=\"linear\")\n",
    "svr.fit(x_train,y_train)\n",
    "svr.score(x_train,y_train)\n",
    "pred_y=svr.predict(x_test)\n",
    "\n",
    "svr=SVR(kernel=\"poly\")\n",
    "svr.fit(x_train,y_train)\n",
    "svr.score(x_train,y_train)\n",
    "pred_y=svr.predict(x_test)\n",
    "\n",
    "svr=SVR(kernel=\"rbf\")\n",
    "svr.fit(x_train,y_train)\n",
    "svr.score(x_train,y_train)\n",
    "pred_y=svr.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advance method :-\n",
    "\n",
    "# In this method we making kernelist all the kernel values (Linear,poly,rbf) into the one list \n",
    "# and making the for loop here, into the kernellist liner,poly,rbf one bye one rotate 'i'values to SVR to checking the score.\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "kernellist=['linear','poly','rbf']\n",
    "for i in kernellist:\n",
    "    sv=SVR(kernel=i)\n",
    "    sv.fit(x_train,y_train)\n",
    "    print(sv.score(x_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOtes :-\n",
    "* Lasso, ridge,elasticness is to surpress the any kind of underfitting and overfitting comming into the model.\n",
    "* Check the all the algorithms ,where our ans is near by to y test predicted with lasso ridge elasticness and try to improve the alpha value also we can tune the alpha value .\n",
    "* with the random state we can do also that.\n",
    "* these are all hyper parameter tuning(alpha value tuning jha tk mujhe samajh aya)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET-SonarClassification_Scaling-ROC_AUCCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('sonar.csv',names=range(0,61),header=0)\n",
    "\n",
    "#sonar dataaet has 60 coloumn so we give range for  column name bcz of dataset has  not given column name header = 0 means for change the name of the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data \n",
    "\n",
    "#we can see in this dataset R=rock and M=mine \n",
    "#and in this dataset has two category rock and mine so we used logisticregression.\n",
    "#here rock and mine are y lable  or target.\n",
    "#in dataset has numerical dataset these are angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,0:-1]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PCA Tecnique Example :-__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=10)\n",
    "#pca=PCA(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This R and M is label encoder into the 0 and 1.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__StanderdScaling Example:-__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale= StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scale.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and testing the data\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.22,random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train,y_train)\n",
    "lr.score(x_train,y_train)\n",
    "predlr=lr.predict(x_test)\n",
    "print(accuracy_score(y_test,predlr))\n",
    "print(confusion_matrix(y_test,predlr))\n",
    "print(classification_report(y_test,predlr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accure = .80 means  80% our model is working is well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb= GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainig the model\n",
    "\n",
    "gnb.fit(x_train,y_train)\n",
    "gnb.score(x_train,y_train)\n",
    "predgnb=gnb.predict(x_test)\n",
    "print(accuracy_score(y_test,predgnb))\n",
    "print(confusion_matrix(y_test,predgnb))\n",
    "print(classification_report(y_test,predgnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traing the model with SVC\n",
    "svc.fit(x_train,y_train)\n",
    "svc.score(x_train,y_train)\n",
    "predsvc=svc.predict(x_test)\n",
    "print(accuracy_score(y_test,predsvc))\n",
    "print(confusion_matrix(y_test,predsvc))\n",
    "print(classification_report(y_test,predsvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt= DecisionTreeClassifier()\n",
    "#dt=DetionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(x_train,y_train)\n",
    "dt.score(x_train,y_train)\n",
    "preddtc=dtc.predict(x_test)\n",
    "print(accuracy_score(y_test,preddtc))\n",
    "print(confusion_matrix(y_test,preddtc))\n",
    "print(classification_report(y_test,preddtc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION :-\n",
    "Cross Validation link-https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here gnb(GaussanNB),svc, decission tree value etc should be present in dataset during working on machine learning process. otherowise it will through error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "gnbscores = cross_val_score(gnb, x,y, cv=5)\n",
    "print(gnbscores)\n",
    "print(gnbscores.mean(),gnbscores.std()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svcscores= cross_val_score(svc, x, y, cv=5)\n",
    "print(svcscores)\n",
    "print(svcscore.mean(),svcscore.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtscores = cross_val_score(dt, x, y, cv = 5 )\n",
    "print (dtscores)\n",
    "print(dtscores.mean(),dtscores.std ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat vid lec for this topic from time 1:32 to write information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis(PCA)- \n",
    "\n",
    "* Certain time happen n number of columns so it is not possible to learn entire columns so when the model is not able to learn entire number of columns so we do call it the curse of dimentionlity reduction. it means there are number of columns which model is not able to learn so we should reduce this number of columns to reduce we PCA method.\n",
    "\n",
    "* Instead of Droping he columns Use PCA method we need not drop the any columns Enternally the PCA will Findout the bad columns which are highly corelated with eachother and just make the average of that columns and make it into one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler :-\n",
    "* Standard Scaler is basically standrdlize the data generally we do this after pca technique .\n",
    "* Standard scaler will make entire values of the data set into the same format/value so that all the data model will learn better these standard value and the prediction is much more better.\n",
    "* Whenever we do standard scaler it will do all the mean near by zero and calculating standard diaviation nearby 1 enternally it will do it. so that mean=0 and SD =1 once all the  data come at this position now it will be well understood by the model.\n",
    "* These are the technique to __preprocess the data__ before sending to the model so that we can get more __good accuracy and more good r2score.__\n",
    "\n",
    "* Link - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minmax scaler -\n",
    "* It is similar to the Standard Scaler.\n",
    "* Inside the minmax scaler all the data goes between the Zero and one means if i have 10 columns by this technique all columns come between the zero and one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concrete_Cement_linearreg (Dataset)-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds=pd.read_csv('Concrete_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat.plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='box', subplots=True,layout=(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show null data\n",
    "sb.heatmap(data.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show multicolinearity\n",
    "sb.heatmap(data.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data ['Age']=np.log(data['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sb.displot?\n",
    "sb.distplot(data['Age']) \n",
    "#log transformation is there just show that skewness should be removed.\n",
    "# and data could be come into the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making distribution plot entirely columns.\n",
    "for i in data.columns:\n",
    "    plt.figure()\n",
    "    sb.distplot(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making he pair plot here\n",
    "sb.pairplot(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp=SimpleImputer(missing_values=0,strategy='mean',axis=0)\n",
    "imp=imp.fit(data)\n",
    "dst=imp.transform(data.values)\n",
    "\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst=pd.DataFrame(dst)\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=dst.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dst.iloc[:,0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,xtest,y_train,y_test=train_test_split(x,y,test_size=.22,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=lm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting value and acrual values', pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrices import mean_squard_error,r2score\n",
    "print(mean_squared_error(pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets make one test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicttest (testvalue):\n",
    "    testvalue=testvalue.reshape(1,-1)\n",
    "    t=lm.predict(test value)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testv=np.array([540.0,136.158676,120.288793,162.0,2.500000,1040.0,676.0,28.0])\n",
    "predicttest(testv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all error value throughing because of Imputer cell above make eda process then check all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice on titanic dataset,wine dataset,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
